{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LVQOsCS5wKle"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4CRrTy0whAv",
        "outputId": "21c41830-2ad2-4acc-8d5b-9fa92080189c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZFF5GWewuL-",
        "outputId": "ece8b1e9-bbb8-4864-fd88-bc3c9a98227e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtjJLPozw90O",
        "outputId": "73dfe060-9e95-4b2f-b1c1-911aaf4b10ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XSIBJyQgwKlg"
      },
      "outputs": [],
      "source": [
        "def get_words (data) :\n",
        "    text = data\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+|t\\.co/\\S+'\n",
        "    email_pattern = r'\\b\\w+@\\w+\\.\\w+\\b'\n",
        "    username_pattern = r'@\\w+'\n",
        "    emoji_pattern = r'[\\U00010000-\\U0010FFFF]'\n",
        "\n",
        "    text_cleaned = re.sub(url_pattern, '', text)\n",
        "    text_cleaned = re.sub(email_pattern, '', text_cleaned)\n",
        "    text_cleaned = re.sub(username_pattern, '', text_cleaned)\n",
        "    text_cleaned = re.sub(emoji_pattern, '', text_cleaned)\n",
        "\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', text_cleaned)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLrXOWfgwKlg",
        "outputId": "5ce99a7c-99f5-42cf-d666-d5d32095b641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'had',\n",
              " 'the',\n",
              " 'best',\n",
              " 'flight',\n",
              " 'with',\n",
              " 'The',\n",
              " 'crew',\n",
              " 'was',\n",
              " 'so',\n",
              " 'friendly',\n",
              " 'and',\n",
              " 'the',\n",
              " 'in',\n",
              " 'flight',\n",
              " 'service',\n",
              " 'was',\n",
              " 'top',\n",
              " 'notch',\n",
              " 'FlyingHigh']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "statement = \"Just had the best flight with @AirlineName! The crew was so friendly, and the in-flight service was top-notch! üåü‚úàÔ∏è #FlyingHigh\"\n",
        "words = get_words(statement)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "dVOWvgMEwKlh"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "tA3Jb2o4wKlh"
      },
      "outputs": [],
      "source": [
        "# importing punctuations\n",
        "import string\n",
        "punctuations = list(string.punctuation)\n",
        "stop += punctuations\n",
        "stop += ['flight','airline','flights','AA', 'aa']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OcdzV2ZuwKli"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C8hS55cdwKli"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "txTcpGwEwKlj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "def get_simple_pos (tag) :\n",
        "    if tag.startswith('J') :\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V') :\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N') :\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R') :\n",
        "        return wordnet.ADV\n",
        "    else :\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qod4pv2YwKlj",
        "outputId": "e445d9ab-642e-4bd2-9630-bfb3d6f24171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "type(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "q0OEVoziwKll"
      },
      "outputs": [],
      "source": [
        "def clean_document (words) :\n",
        "    # it should not be a stop word and we have to lemmatize it by getting pos tag\n",
        "    output_words = []\n",
        "    for w in words :\n",
        "        if w.lower() not in stop :\n",
        "            pos = pos_tag([w])\n",
        "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
        "            output_words.append(clean_word)\n",
        "    return output_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "5u-b5f2BwKll"
      },
      "outputs": [],
      "source": [
        "input = clean_document(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = ' '.join(input)"
      ],
      "metadata": {
        "id": "ohxDvWcXylxg"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ox3dK7E9zw8H",
        "outputId": "473b37f3-efc8-4e9e-bbbb-691f22180901"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best crew friendly service top notch FlyingHigh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('countVectorizer.pkl', 'rb') as file:\n",
        "    count_vec = pickle.load(file)"
      ],
      "metadata": {
        "id": "7mzJDwagxESz"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_new = count_vec.transform([input_str]).toarray()"
      ],
      "metadata": {
        "id": "5hg3gy71xnPP"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogRyQKpyz6i_",
        "outputId": "0b61b15d-1e4a-4371-e99f-39cd8f4bf7b2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 7079)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model.pkl', 'rb') as file :\n",
        "    model = pickle.load(file)"
      ],
      "metadata": {
        "id": "gIcjwNJKx1S8"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(input_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeNrfGf5x-d0",
        "outputId": "8d4c7d77-5098-44d7-9d79-8112019dbc90"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}